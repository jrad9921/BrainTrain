import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import nibabel as nib
from dataloaders import dataloader
from architectures import sfcn_cls, sfcn_ssl2, head, lora_layers
# Import configuration
import config as cfg


def load_model(model_path, device):
    """Load trained model"""
    # Create model architecture based on training mode
    if cfg.TRAINING_MODE == 'sfcn':
        model = sfcn_cls.SFCN(output_dim=cfg.N_CLASSES).to(device)
    
    elif cfg.TRAINING_MODE in ['linear', 'ssl-finetuned', 'lora']:
        backbone = sfcn_ssl2.SFCN()
        
        # For LoRA, apply LoRA layers before loading weights
        if cfg.TRAINING_MODE == 'lora':
            backbone = lora_layers.apply_lora_to_model(
                backbone,
                rank=cfg.LORA_RANK,
                alpha=cfg.LORA_ALPHA,
                target_modules=cfg.LORA_TARGET_MODULES
            )
        
        model = head.ClassifierHeadMLP_(backbone, output_dim=cfg.N_CLASSES).to(device)
    
    else:
        raise ValueError(f"Invalid TRAINING_MODE: {cfg.TRAINING_MODE}")
    
    # Load checkpoint
    print(f"Loading model from: {model_path}")
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    # Handle different checkpoint formats
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        elif 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
        else:
            model.load_state_dict(checkpoint)
    else:
        model.load_state_dict(checkpoint)
    
    print(f"Successfully loaded {cfg.TRAINING_MODE} model")
    model.eval()
    return model


def find_last_conv_layer(model):
    """Find the last convolutional layer in the model"""
    last_conv = None
    last_conv_name = None
    
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv3d):
            last_conv = module
            last_conv_name = name
    
    if last_conv is None:
        raise ValueError("No Conv3d layer found in model!")
    
    print(f"Using layer for GradCAM: {last_conv_name}")
    return last_conv


def compute_gradcam(model, image, target_layer, target='logit_diff', class_idx=None, mode='magnitude'):
    """
    Compute GradCAM attention map using intermediate layer activations.
    
    Args:
        model: PyTorch model
        image: Input image tensor [1,1,D,H,W]
        target_layer: Layer to compute gradients for
        target: 'logit_diff' (default), 'pred', or 'target_class'
        class_idx: Target class index (only used if target='target_class')
        mode: 'magnitude' (ReLU applied) or 'signed' (preserve sign)
    
    Returns:
        gradcam: Attention map [D,H,W]
        pred_class: Predicted class
        confidence: Prediction confidence
    """
    activations = []
    gradients = []
    
    def forward_hook(module, input, output):
        activations.append(output.detach())
    
    def backward_hook(module, grad_input, grad_output):
        gradients.append(grad_output[0].detach())
    
    # Register hooks
    handle_f = target_layer.register_forward_hook(forward_hook)
    handle_b = target_layer.register_full_backward_hook(backward_hook)
    
    try:
        # Forward pass
        model.zero_grad(set_to_none=True)
        logits = model(image)
        probs = torch.softmax(logits, dim=1)
        
        # Determine target and get predictions
        if target == 'logit_diff' and logits.shape[1] == 2:
            score = (logits[0, 1] - logits[0, 0])
            pred_class = int(probs[0, 1] > probs[0, 0])
            conf = probs[0, pred_class].item()
        elif target == 'pred':
            pred_class = torch.argmax(probs, dim=1).item()
            score = logits[0, pred_class]
            conf = probs[0, pred_class].item()
        elif target == 'target_class':
            if class_idx is None:
                raise ValueError("Must specify class_idx when target='target_class'")
            pred_class = torch.argmax(probs, dim=1).item()
            score = logits[0, class_idx]
            conf = probs[0, class_idx].item()
        else:
            raise ValueError(f"Unknown target: {target}")
        
        # Backward pass
        score.backward()
        
        # Compute GradCAM
        act = activations[0]  # [1, C, D, H, W]
        grad = gradients[0]   # [1, C, D, H, W]
        
        # Global average pooling of gradients
        weights = grad.mean(dim=(2, 3, 4), keepdim=True)  # [1, C, 1, 1, 1]
        
        # Weighted combination of activation maps
        cam = (weights * act).sum(dim=1, keepdim=True)  # [1, 1, D, H, W]
        
        # Apply mode
        if mode == 'magnitude':
            cam = F.relu(cam)
        # else: keep signed values (mode == 'signed')
        
        cam = cam.squeeze().cpu().numpy()  # [D, H, W]
        
        # Normalize
        if mode == 'magnitude':
            # Normalize to [0, 1]
            if cam.max() > 0:
                cam = (cam - cam.min()) / (cam.max() - cam.min())
        else:  # signed
            # Normalize to [-1, 1] while preserving sign
            max_abs = np.abs(cam).max()
            if max_abs > 0:
                cam = cam / max_abs
        
        return cam, pred_class, conf
    
    finally:
        handle_f.remove()
        handle_b.remove()


def compute_saliency(model, image, target='logit_diff', class_idx=None, mode='magnitude'):
    """
    Compute saliency map (gradient of output w.r.t. input).
    
    Args:
        model: PyTorch model
        image: Input image tensor [1,1,D,H,W]
        target: 'logit_diff' (default), 'pred', or 'target_class'
        class_idx: Target class index (only used if target='target_class')
        mode: 'magnitude' (absolute value) or 'signed' (preserve sign)
    
    Returns:
        saliency: Attention map [D,H,W]
        pred_class: Predicted class
        confidence: Prediction confidence
    """
    image.requires_grad = True
    model.zero_grad(set_to_none=True)
    
    logits = model(image)
    probs = torch.softmax(logits, dim=1)
    
    # Determine target
    if target == 'logit_diff' and logits.shape[1] == 2:
        score = (logits[0, 1] - logits[0, 0])
        pred_class = int(probs[0, 1] > probs[0, 0])
        conf = probs[0, pred_class].item()
    elif target == 'pred':
        pred_class = torch.argmax(probs, dim=1).item()
        score = logits[0, pred_class]
        conf = probs[0, pred_class].item()
    elif target == 'target_class':
        if class_idx is None:
            raise ValueError("Must specify class_idx when target='target_class'")
        pred_class = torch.argmax(probs, dim=1).item()
        score = logits[0, class_idx]
        conf = probs[0, class_idx].item()
    else:
        raise ValueError(f"Unknown target: {target}")
    
    # Backward
    score.backward()
    
    # Get saliency map
    if mode == 'magnitude':
        saliency = image.grad.data.abs().squeeze().cpu().numpy()  # [D, H, W]
        # Normalize to [0, 1]
        if saliency.max() > 0:
            saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())
    else:  # signed
        saliency = image.grad.data.squeeze().cpu().numpy()  # [D, H, W]
        # Normalize to [-1, 1] while preserving sign
        max_abs = np.abs(saliency).max()
        if max_abs > 0:
            saliency = saliency / max_abs
    
    image.requires_grad = False
    return saliency, pred_class, conf


def load_image(path, device):
    """Load and preprocess image"""
    img_data = np.load(path)
    
    # Display version (8-bit for overlays)
    p1, p99 = np.percentile(img_data, (1, 99))
    img_np = np.clip(img_data, p1, p99)
    img_np = ((img_np - img_np.min()) / max(img_np.max() - img_np.min(), 1e-8) * 255).astype(np.uint8)
    
    # Model input version
    img_t = torch.from_numpy(img_data).float().unsqueeze(0).unsqueeze(0).to(device)
    return img_t, img_np


def save_visualization(heatmap, image, name, output_dir, signed=False):
    """Save heatmap visualization as overlays"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Select slices
    D, H, W = image.shape
    slices = {
        'axial': D // 2,
        'coronal': H // 2,
        'sagittal': W // 2
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for idx, (view, slice_idx) in enumerate(slices.items()):
        ax = axes[idx]
        
        if view == 'axial':
            img_slice = image[slice_idx, :, :]
            heat_slice = heatmap[slice_idx, :, :]
        elif view == 'coronal':
            img_slice = image[:, slice_idx, :]
            heat_slice = heatmap[:, slice_idx, :]
        else:  # sagittal
            img_slice = image[:, :, slice_idx]
            heat_slice = heatmap[:, :, slice_idx]
        
        # Display image
        ax.imshow(img_slice.T, cmap='gray', origin='lower')
        
        # Overlay heatmap
        if signed:
            im = ax.imshow(heat_slice.T, cmap='RdBu_r', alpha=0.5, 
                          vmin=-np.abs(heat_slice).max(), vmax=np.abs(heat_slice).max(),
                          origin='lower')
        else:
            im = ax.imshow(heat_slice.T, cmap='hot', alpha=0.5, 
                          vmin=0, vmax=heat_slice.max(),
                          origin='lower')
        
        ax.set_title(f"{view.capitalize()} (slice {slice_idx})", fontsize=12)
        ax.axis('off')
    
    plt.colorbar(im, ax=axes, fraction=0.046, pad=0.04)
    plt.suptitle(name, fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    save_path = os.path.join(output_dir, f"{name}.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Saved: {save_path}")


def generate_heatmaps(heatmap_dir, attention_method='gradcam', 
                      attention_mode='magnitude', mode='top_individual', top_n=5, 
                      attention_target='logit_diff', attention_class_idx=None):
    """
    Main function to generate heatmaps
    
    Args:
        heatmap_dir: Directory to save heatmap visualizations in png and quantitative regional analysis in csv 
        attention_method: 'gradcam' or 'saliency'
        attention_mode: 'magnitude' or 'signed'
        mode: 'single', 'average', or 'top_individual'
        top_n: Number of top samples to visualize
        attention_target: 'logit_diff', 'pred', or 'target_class'
        attention_class_idx: Target class index (only for 'target_class')
    """
    device = cfg.DEVICE
    
    print("\n" + "="*70)
    print("HEATMAP GENERATION")
    print("="*70)
    print(f"Test cohort: {cfg.TEST_COHORT}")
    print(f"Attention method: {attention_method}")
    print(f"Attention mode: {attention_mode}")
    print(f"Visualization mode: {mode}")
    print(f"Top N: {top_n}")
    
    # Create output directories
    os.makedirs(heatmap_dir, exist_ok=True)
    model_path=f'{cfg.MODEL_DIR}/{cfg.TRAINING_MODE}/{cfg.EXPERIMENT_NAME}.pth'

    # Load model
    model = load_model(model_path, device)
    
    # Get target layer for GradCAM
    if attention_method == 'gradcam':
        target_layer = find_last_conv_layer(model)
    # Load CSV data
    df = pd.read_csv(cfg.CSV_TEST)
    print(f"Test dataset size: {len(df)}")
    # Create test dataset
    test_dataset = dataloader.BrainDataset(
        csv_file=cfg.CSV_TEST,
        root_dir=cfg.TENSOR_DIR_TEST,
        column_name=cfg.COLUMN_NAME,
        num_rows=None,
        num_classes=cfg.N_CLASSES,
        task='classification'
    )
    
    print(f"Test dataset size: {len(test_dataset)}")
    
    # Process images
    results = []
    signed = (attention_mode == 'signed')
    
    print("\nGenerating heatmaps...")
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
        eid = str(row['eid'])
        label = int(row[cfg.COLUMN_NAME])
        
        # Construct image path
        image_path = os.path.join(cfg.TENSOR_DIR_TEST, f"{eid}.npy")
        
        # Check if file exists
        if not os.path.exists(image_path):
            print(f"Warning: Image not found: {image_path}")
            continue
        
        # Load image
        image_t, image_np = load_image(image_path, device)
        
        # Compute attention
        if attention_method == 'gradcam':
            att_map, pred_class, confidence = compute_gradcam(
                model, image_t, target_layer,
                target=attention_target,
                class_idx=attention_class_idx,
                mode=attention_mode
            )
        else:  # saliency
            att_map, pred_class, confidence = compute_saliency(
                model, image_t,
                target=attention_target,
                class_idx=attention_class_idx,
                mode=attention_mode
            )
        
        results.append({
            'heatmap': att_map,
            'image': image_np,
            'confidence': confidence,
            'pred_class': pred_class,
            'true_class': label,
            'eid': eid
        })
        
        if mode == 'single':
            break
    
    # Generate visualizations
    print("\nCreating visualizations...")
    
    if mode == 'single':
        result = results[0]
        save_visualization(
            result['heatmap'], result['image'],
            f"single_{result['eid']}_pred{result['pred_class']}_conf{result['confidence']:.3f}",
            heatmap_dir,
            signed=signed
        )
    
    elif mode == 'average':
        top_results = sorted(results, key=lambda x: x['confidence'], reverse=True)[:top_n]
        avg_heatmap = np.mean([r['heatmap'] for r in top_results], axis=0)
        save_visualization(
            avg_heatmap, results[0]['image'], 
            f"average_top{top_n}",
            heatmap_dir, 
            signed=signed
        )
    
    elif mode == 'top_individual':
        # Get top confident predictions for class 1
        positive_results = [r for r in results if r['pred_class'] == 1]
        top_positive = sorted(positive_results, key=lambda x: x['confidence'], reverse=True)[:top_n]
        
        for i, result in enumerate(top_positive):
            save_visualization(
                result['heatmap'], result['image'],
                f"top{i+1}_{result['eid']}_conf{result['confidence']:.3f}",
                heatmap_dir,
                signed=signed
            )
    
    # Save summary
    summary_df = pd.DataFrame([{
        'eid': r['eid'],
        'pred_class': r['pred_class'],
        'true_class': r['true_class'],
        'confidence': r['confidence']
    } for r in results])
    
    summary_path = os.path.join(heatmap_dir, 'heatmap_summary.csv')
    summary_df.to_csv(summary_path, index=False)
    print(f"\nSummary saved to: {summary_path}")
    print(f"Heatmaps saved to: {heatmap_dir}")
    print("\nHeatmap generation completed!")
    
    return results


def main():
    """Main function"""
    # Setup
    if torch.cuda.is_available():
        torch.cuda.set_device(cfg.DEVICE)
    torch.manual_seed(42)
    
    print("\n" + "="*70)
    print("HEATMAP CONFIGURATION")
    print("="*70)
    print(f"Training mode: {cfg.TRAINING_MODE}")
    print(f"Model: {cfg.MODEL_DIR}")
    print(f"Test cohort: {cfg.TEST_COHORT}")
    print(f"Test CSV: {cfg.CSV_TEST}")
    print("="*70)
    
    # Set heatmap parameters (modify these as needed)
    attention_method = cfg.ATTENTION_METHOD      # Options: 'gradcam', 'saliency'
    attention_mode = cfg.ATTENTION_MODE      # Options: 'magnitude', 'signed'
    mode = cfg.HEATMAP_MODE       # Options: 'single', 'average', 'top_individual'
    top_n = cfg.HEATMAP_TOP_N
    explainability_path = f'{cfg.COLUMN_NAME}/{cfg.TEST_COHORT}/{cfg.TRAINING_MODE}/{cfg.ATTENTION_METHOD}/{cfg.ATTENTION_MODE}/{cfg.EXPERIMENT_NAME}'
    
    # Create output directories
    heatmap_dir = os.path.join(cfg.EXPLAINABILITY_DIR, explainability_path)
    
    # Generate heatmaps
    generate_heatmaps(
        heatmap_dir=heatmap_dir,
        attention_method=attention_method,
        attention_mode=attention_mode,
        mode=mode,
        top_n=top_n,
        attention_target='logit_diff',
        attention_class_idx=None
    )
    
    print("\n✓ All done!")


if __name__ == "__main__":
    main()

if __name__ == "__main__":
    main()
